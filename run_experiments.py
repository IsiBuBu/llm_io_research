# run_experiments.py

import asyncio
import logging
import argparse
import json
import sys
from pathlib import Path
from datetime import datetime
from collections import defaultdict
from typing import Dict, List, Any
import numpy as np

# Ensure the project root is in the Python path
sys.path.append(str(Path(__file__).parent.parent))

from config.config import (
    get_all_game_configs,
    get_challenger_models,
    get_defender_model,
    get_model_display_name,
    get_experiments_dir,
    get_simulation_count,
    get_data_dir
)
from games import create_game
from agents import create_agent, AgentResponse
from metrics.metric_utils import GameResult, create_game_result

# Helper function for NPV calculation
def calculate_npv(profit_stream: List[float], discount_factor: float) -> float:
    """Calculates the Net Present Value of a stream of profits."""
    return sum(profit * (discount_factor ** t) for t, profit in enumerate(profit_stream))

class Competition:
    """Orchestrates a series of game simulations between LLM agents."""

    def __init__(self, challenger_models: List[str], defender_model: str, mock_mode: bool = False):
        self.challenger_models = challenger_models
        self.defender_model = defender_model
        self.mock_mode = mock_mode
        self.output_dir = get_experiments_dir()
        self.logger = logging.getLogger(self.__class__.__name__)

        try:
            dataset_path = get_data_dir() / "master_datasets.json"
            with open(dataset_path, 'r') as f:
                self.master_datasets = json.load(f)
            self.logger.info("Successfully loaded master datasets for reproducible experiments.")
        except FileNotFoundError:
            self.logger.critical(f"❌ CRITICAL ERROR: Input file not found at '{dataset_path}'.")
            self.logger.critical("Please run the data generation script first by executing: python generate_data.py")
            sys.exit(1)

    async def run_all_experiments(self):
        """Runs the full suite of experiments as defined in the config."""
        self.logger.info("=" * 80)
        self.logger.info("STARTING EXPERIMENT SUITE")
        if self.mock_mode: self.logger.info("🎭 RUNNING IN MOCK MODE 🎭")
        self.logger.info("=" * 80)

        all_game_names = ['salop', 'green_porter', 'spulber', 'athey_bagwell']

        for game_name in all_game_names:
            game_configs = get_all_game_configs(game_name)
            for config in game_configs:
                for challenger_model in self.challenger_models:
                    self.logger.info(f"Running: [{game_name}]-[{config.condition_name}] for Challenger: [{get_model_display_name(challenger_model)}]")

                    num_simulations = get_simulation_count(config.experiment_type)
                    tasks = [self.run_single_simulation(challenger_model, config, sim_id) for sim_id in range(num_simulations)]
                    simulation_results = await asyncio.gather(*tasks)

                    self._save_competition_result(challenger_model, config, simulation_results)

    async def run_single_simulation(self, challenger_model: str, config, sim_id: int):
        """Runs a single simulation of a game."""
        game = create_game(config.game_name)
        game_state = game.initialize_game_state(config, sim_id)
        game_state['simulation_id'] = sim_id

        dataset_key = f"{config.game_name}_{config.experiment_type}_{config.condition_name}"
        if dataset_key in self.master_datasets:
            dataset = self.master_datasets[dataset_key]
            
            if config.game_name == 'spulber' and 'player_private_costs' in dataset:
                costs_data = dataset.get('player_private_costs', {})
                sim_costs = {}
                for player, costs_list in costs_data.items():
                    if isinstance(costs_list, list) and len(costs_list) > sim_id:
                        sim_costs[player] = costs_list[sim_id]
                    else:
                        sim_costs[player] = costs_list
                game_state['player_private_costs'] = sim_costs
            
            else:
                sim_specific_data = {}
                for key, value in dataset.items():
                    if isinstance(value, list) and len(value) > sim_id:
                        sim_specific_data[key] = value[sim_id]
                    elif isinstance(value, dict):
                        player_data = {}
                        for player, data in value.items():
                            if isinstance(data, list) and len(data) > sim_id:
                                player_data[player] = data[sim_id]
                            else:
                                player_data[player] = data
                        sim_specific_data[key] = player_data
                game_state['predefined_sequences'] = sim_specific_data


        agents = {
            'challenger': create_agent(challenger_model, 'challenger', agent_type='experiment', mock_mode=self.mock_mode),
            **{f'defender_{i+1}': create_agent(self.defender_model, f'defender_{i+1}', agent_type='experiment', mock_mode=self.mock_mode)
               for i in range(config.constants['number_of_players'] - 1)}
        }

        if hasattr(game, 'update_game_state'):
            return await self._run_dynamic_game(game, agents, config, game_state, challenger_model)
        else:
            return await self._run_static_game(game, agents, config, game_state, challenger_model)

    async def _run_static_game(self, game, agents, config, game_state, challenger_model: str):
        actions, responses = await self._get_all_actions(game, agents, config, game_state)
        payoffs = game.calculate_payoffs(actions, config, game_state)
        game_data = game.get_game_data_for_logging(actions, payoffs, config, game_state)

        # FIX: Re-add the llm_metadata to the game_data for static games
        game_data['llm_metadata'] = {pid: resp.__dict__ for pid, resp in responses.items()}

        return create_game_result(game_state['simulation_id'], config.game_name, config.experiment_type, config.condition_name, challenger_model, list(agents.keys()), actions, payoffs, game_data)

    async def _run_dynamic_game(self, game, agents, config, game_state, challenger_model: str):
        """Dispatcher for dynamic games to their specialized workflows."""
        if config.game_name == 'athey_bagwell':
            return await self._run_athey_bagwell_game(game, agents, config, game_state, challenger_model)
        
        if config.game_name == 'green_porter':
            return await self._run_green_porter_game(game, agents, config, game_state, challenger_model)
        
        # Fallback for any other dynamic games
        self.logger.warning(f"No specialized workflow for dynamic game '{config.game_name}'. Using generic loop.")
        time_horizon = config.constants.get('time_horizon', 50)
        all_rounds_data = []

        for _ in range(time_horizon):
            actions, responses = await self._get_all_actions(game, agents, config, game_state)
            payoffs = game.calculate_payoffs(actions, config, game_state)
            round_data = game.get_game_data_for_logging(actions, payoffs, config, game_state)
            
            all_rounds_data.append(round_data)
            game_state = game.update_game_state(game_state, actions, config, payoffs)

        # --- FINAL CALCULATIONS (FALLBACK) ---
        profit_streams = defaultdict(list)
        for round_data in all_rounds_data:
            profits = round_data.get('payoffs', {})
            for p_id, profit in profits.items():
                profit_streams[p_id].append(profit)

        final_npvs = {p_id: calculate_npv(stream, config.constants.get('discount_factor', 0.95)) for p_id, stream in profit_streams.items()}

        game_data = { "constants": config.constants, "rounds": all_rounds_data, "final_npvs": final_npvs }
        
        return create_game_result(game_state['simulation_id'], config.game_name, config.experiment_type, config.condition_name, challenger_model, list(agents.keys()), {}, final_npvs, game_data)

    async def _run_green_porter_game(self, game, agents, config, game_state, challenger_model: str):
        """Runs the specialized workflow for Green & Porter, logging both strategic and mechanical rounds."""
        time_horizon = config.constants.get('time_horizon', 50)
        all_rounds_data = []

        while game_state['current_period'] <= time_horizon:
            if game_state['market_state'] == 'Collusive':
                actions, responses = await self._get_all_actions(game, agents, config, game_state)
            else:
                cournot_quantity = config.constants.get('cournot_quantity')
                actions = {pid: {'quantity': cournot_quantity} for pid in agents}
                responses = {}
            
            payoffs = game.calculate_payoffs(actions, config, game_state)
            round_data = game.get_game_data_for_logging(actions, payoffs, config, game_state)
            
            if responses:
                round_data['llm_metadata'] = {pid: resp.__dict__ for pid, resp in responses.items()}
            
            all_rounds_data.append(round_data)
            game_state = game.update_game_state(game_state, actions, config, payoffs)

        # --- FINAL CALCULATIONS (GREEN & PORTER) ---
        profit_streams = defaultdict(list)
        for round_data in all_rounds_data:
            profits = round_data.get('payoffs', {})
            for p_id, profit in profits.items():
                profit_streams[p_id].append(profit)
        
        final_npvs = {p_id: calculate_npv(stream, config.constants.get('discount_factor', 0.95)) for p_id, stream in profit_streams.items()}
        
        state_history = [r.get('market_state', 'Unknown') for r in all_rounds_data]
        reversion_triggers = sum(1 for i in range(len(state_history) - 1) if state_history[i] == 'Collusive' and state_history[i+1] == 'Reversionary')

        game_data = {
            "constants": config.constants, "rounds": all_rounds_data, "final_npvs": final_npvs,
            "reversion_frequency": reversion_triggers / (time_horizon - 1) if time_horizon > 1 else 0,
            "state_history": state_history
        }

        return create_game_result(game_state['simulation_id'], config.game_name, config.experiment_type, config.condition_name, challenger_model, list(agents.keys()), {}, final_npvs, game_data)


    async def _run_athey_bagwell_game(self, game, agents, config, game_state, challenger_model: str):
        """
        Runs the specialized workflow for Athey & Bagwell, correctly modeling
        the sequential odd and even periods as described in the paper.
        """
        time_horizon = config.constants.get('time_horizon', 50)
        all_rounds_data = []
        last_period_reports = {}

        for period in range(1, time_horizon + 1):
            game_state['current_period'] = period
            
            is_odd_period = (period % 2 != 0)

            if is_odd_period:
                # ODD PERIOD: Strategic reporting with an LLM call
                reports, responses = await self._get_all_actions(game, agents, config, game_state, stage=1)
                last_period_reports = reports  # Save reports for the next even period

                # Determine market share based on this period's reports
                low_reporters = [pid for pid, action in reports.items() if action.get('report') == 'low']
                market_shares = {}
                if len(low_reporters) == 1:
                    market_shares = {pid: (1.0 if pid == low_reporters[0] else 0.0) for pid in agents}
                else:
                    share = 1.0 / len(agents)
                    market_shares = {pid: share for pid in agents}
                
                actions_for_payoff = {pid: {'quantity': share} for pid, share in market_shares.items()}
                payoffs = game.calculate_payoffs(actions_for_payoff, config, game_state)
                
                # Log the outcome of the strategic round
                round_data = game.get_game_data_for_logging(reports, payoffs, config, game_state)
                round_data['llm_metadata'] = {pid: resp.__dict__ for pid, resp in responses.items()}
                all_rounds_data.append(round_data)

            else:  # EVEN PERIOD: Mechanical enforcement, no LLM call
                # Determine market share based on the *previous* odd period's reports
                low_reporters = [pid for pid, action in last_period_reports.items() if action.get('report') == 'low']
                high_reporters = [pid for pid in agents if pid not in low_reporters]
                
                market_shares = {}
                if high_reporters:
                    share = 1.0 / len(high_reporters)
                    market_shares = {pid: (share if pid in high_reporters else 0.0) for pid in agents}
                else: # If everyone reported low, they all get punished with an equal share
                    market_shares = {pid: 1.0 / len(agents) for pid in agents}

                actions = {pid: {'quantity': share} for pid, share in market_shares.items()}
                payoffs = game.calculate_payoffs(actions, config, game_state)
                
                # Log the outcome of the mechanical enforcement round
                round_data = game.get_game_data_for_logging(actions, payoffs, config, game_state)
                all_rounds_data.append(round_data)

        # --- FINAL CALCULATIONS (ATHEY & BAGWELL) ---
        profit_streams = defaultdict(list)
        for round_data in all_rounds_data:
            profits = round_data.get('payoffs', {})
            for p_id, profit in profits.items():
                profit_streams[p_id].append(profit)

        final_npvs = {p_id: calculate_npv(stream, config.constants.get('discount_factor', 0.95)) for p_id, stream in profit_streams.items()}
        hhi_per_round = [sum((s * 100) ** 2 for s in r.get('game_outcomes', {}).get('player_market_shares', {}).values()) for r in all_rounds_data]

        game_data = {
            "constants": config.constants, "rounds": all_rounds_data, "final_npvs": final_npvs,
            "average_hhi": np.mean(hhi_per_round) if hhi_per_round else 0
        }
        
        return create_game_result(game_state['simulation_id'], config.game_name, config.experiment_type, config.condition_name, challenger_model, list(agents.keys()), {}, final_npvs, game_data)

    async def _get_all_actions(self, game, agents, config, game_state, stage: int = 1) -> (Dict[str, Any], Dict[str, AgentResponse]):
        """Gets actions and full responses from all agents concurrently."""
        call_id = f"{config.game_name}-{game_state.get('simulation_id', 'N/A')}"

        prompts = {pid: game.generate_player_prompt(pid, game_state, config) for pid in agents}
        
        tasks = {pid: agent.get_response(prompts[pid], call_id, config) for pid, agent in agents.items()}
        responses = await asyncio.gather(*tasks.values())
        response_map = dict(zip(agents.keys(), responses))
        
        actions = {pid: game.parse_llm_response(resp.content, pid, call_id, stage=stage) or {} for pid, resp in response_map.items()}

        return actions, response_map

    def _save_competition_result(self, challenger, config, results):
        """Saves the results of a set of simulations to a JSON file."""
        challenger_dir = self.output_dir / config.game_name / challenger
        challenger_dir.mkdir(parents=True, exist_ok=True)
        filepath = challenger_dir / f"{config.condition_name}_competition_result.json"

        dict_results = [res.__dict__ for res in results]

        output_data = {
            "game_name": config.game_name, "experiment_type": config.experiment_type,
            "condition_name": config.condition_name, "challenger_model": challenger,
            "simulation_results": dict_results
        }

        with open(filepath, 'w') as f:
            json.dump(output_data, f, indent=2)
        self.logger.info(f"Saved results to {filepath}")

def setup_logging(verbose: bool, mock_mode: bool):
    """Configures logging to both console and a timestamped file."""
    logs_dir = Path("logs")
    logs_dir.mkdir(exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = logs_dir / f"experiment_{timestamp}{'_mock' if mock_mode else ''}.log"

    logging.basicConfig(level=logging.DEBUG if verbose else logging.INFO,
                        format='%(asctime)s - %(name)-20s - %(levelname)-8s - %(message)s',
                        handlers=[logging.FileHandler(log_file), logging.StreamHandler(sys.stdout)])
    logging.info(f"Logging initialized. Log file at: {log_file}")

def parse_arguments():
    """Parses command-line arguments."""
    parser = argparse.ArgumentParser(description="LLM Game Theory Experiment Runner")
    parser.add_argument('--mock', action='store_true', help="Run in mock mode with random agents.")
    parser.add_argument('-v', '--verbose', action='store_true', help="Enable verbose DEBUG logging.")
    return parser.parse_args()

async def main():
    """Main entry point for running the experiments."""
    args = parse_arguments()
    setup_logging(args.verbose, args.mock)

    competition = Competition(get_challenger_models(), get_defender_model(), mock_mode=args.mock)
    await competition.run_all_experiments()

if __name__ == "__main__":
    asyncio.run(main())
